{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb325e7-42d1-458a-87be-280360bb7131",
   "metadata": {},
   "source": [
    "Credit card fraud detection model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522aa967-c626-4611-8161-b90fddba6b80",
   "metadata": {},
   "source": [
    "3 types of classification model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da121b1-3620-410d-a903-7f93bd025c73",
   "metadata": {},
   "source": [
    "1. Without Feature Selection and without data balancing\n",
    "2. With Feature Selection and without data balancing\n",
    "3. With Feature Selection and with data balancing\n",
    "4. Without Feature Selection and with data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6106ebed-7706-4c7a-8a81-3ff3c2baf29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: matplotlib in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: seaborn in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (0.13.2)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.0-1-cp310-cp310-win_amd64.whl (10.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (from matplotlib) (4.48.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.12.0-cp310-cp310-win_amd64.whl (46.2 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\projects\\ml\\credit_fraud_detection\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.0 scipy-1.12.0 threadpoolctl-3.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ad81d54-bba1-499e-86c3-f430940cfc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prath\\AppData\\Local\\Temp\\ipykernel_30504\\1494754143.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48855a02-e6a5-48c7-b90a-f23ee601a262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing and analysing the dataset\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff27df3-6ed7-4670-a629-dba52775520f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a78af90b-9841-4b5b-a60e-b8fc23b323af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"Class\"].value_counts().plot(kind= \"pie\",autopct='%1.2f%%', shadow = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3053cd7-e9aa-4fa6-9b0e-fef0cb2c794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "legit = df[df['Class'] == 0]\n",
    "fraud = df[df['Class'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a45f1cd-77d2-4364-bb1a-da3c613def7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.000000</td>\n",
       "      <td>284315.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94838.202258</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>-0.007860</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.009637</td>\n",
       "      <td>-0.000987</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001235</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.000295</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>88.291022</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47484.015786</td>\n",
       "      <td>1.929814</td>\n",
       "      <td>1.636146</td>\n",
       "      <td>1.459429</td>\n",
       "      <td>1.399333</td>\n",
       "      <td>1.356952</td>\n",
       "      <td>1.329913</td>\n",
       "      <td>1.178812</td>\n",
       "      <td>1.161283</td>\n",
       "      <td>1.089372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716743</td>\n",
       "      <td>0.723668</td>\n",
       "      <td>0.621541</td>\n",
       "      <td>0.605776</td>\n",
       "      <td>0.520673</td>\n",
       "      <td>0.482241</td>\n",
       "      <td>0.399847</td>\n",
       "      <td>0.329570</td>\n",
       "      <td>250.105092</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-56.407510</td>\n",
       "      <td>-72.715728</td>\n",
       "      <td>-48.325589</td>\n",
       "      <td>-5.683171</td>\n",
       "      <td>-113.743307</td>\n",
       "      <td>-26.160506</td>\n",
       "      <td>-31.764946</td>\n",
       "      <td>-73.216718</td>\n",
       "      <td>-6.290730</td>\n",
       "      <td>...</td>\n",
       "      <td>-34.830382</td>\n",
       "      <td>-10.933144</td>\n",
       "      <td>-44.807735</td>\n",
       "      <td>-2.836627</td>\n",
       "      <td>-10.295397</td>\n",
       "      <td>-2.604551</td>\n",
       "      <td>-22.565679</td>\n",
       "      <td>-15.430084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54230.000000</td>\n",
       "      <td>-0.917544</td>\n",
       "      <td>-0.599473</td>\n",
       "      <td>-0.884541</td>\n",
       "      <td>-0.850077</td>\n",
       "      <td>-0.689398</td>\n",
       "      <td>-0.766847</td>\n",
       "      <td>-0.551442</td>\n",
       "      <td>-0.208633</td>\n",
       "      <td>-0.640412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228509</td>\n",
       "      <td>-0.542403</td>\n",
       "      <td>-0.161702</td>\n",
       "      <td>-0.354425</td>\n",
       "      <td>-0.317145</td>\n",
       "      <td>-0.327074</td>\n",
       "      <td>-0.070852</td>\n",
       "      <td>-0.052950</td>\n",
       "      <td>5.650000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84711.000000</td>\n",
       "      <td>0.020023</td>\n",
       "      <td>0.064070</td>\n",
       "      <td>0.182158</td>\n",
       "      <td>-0.022405</td>\n",
       "      <td>-0.053457</td>\n",
       "      <td>-0.273123</td>\n",
       "      <td>0.041138</td>\n",
       "      <td>0.022041</td>\n",
       "      <td>-0.049964</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029821</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>-0.011147</td>\n",
       "      <td>0.041082</td>\n",
       "      <td>0.016417</td>\n",
       "      <td>-0.052227</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.011199</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139333.000000</td>\n",
       "      <td>1.316218</td>\n",
       "      <td>0.800446</td>\n",
       "      <td>1.028372</td>\n",
       "      <td>0.737624</td>\n",
       "      <td>0.612181</td>\n",
       "      <td>0.399619</td>\n",
       "      <td>0.571019</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.598230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185626</td>\n",
       "      <td>0.528407</td>\n",
       "      <td>0.147522</td>\n",
       "      <td>0.439869</td>\n",
       "      <td>0.350594</td>\n",
       "      <td>0.240671</td>\n",
       "      <td>0.090573</td>\n",
       "      <td>0.077962</td>\n",
       "      <td>77.050000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930</td>\n",
       "      <td>18.902453</td>\n",
       "      <td>9.382558</td>\n",
       "      <td>16.875344</td>\n",
       "      <td>34.801666</td>\n",
       "      <td>73.301626</td>\n",
       "      <td>120.589494</td>\n",
       "      <td>18.709255</td>\n",
       "      <td>15.594995</td>\n",
       "      <td>...</td>\n",
       "      <td>22.614889</td>\n",
       "      <td>10.503090</td>\n",
       "      <td>22.528412</td>\n",
       "      <td>4.584549</td>\n",
       "      <td>7.519589</td>\n",
       "      <td>3.517346</td>\n",
       "      <td>31.612198</td>\n",
       "      <td>33.847808</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time             V1             V2             V3  \\\n",
       "count  284315.000000  284315.000000  284315.000000  284315.000000   \n",
       "mean    94838.202258       0.008258      -0.006271       0.012171   \n",
       "std     47484.015786       1.929814       1.636146       1.459429   \n",
       "min         0.000000     -56.407510     -72.715728     -48.325589   \n",
       "25%     54230.000000      -0.917544      -0.599473      -0.884541   \n",
       "50%     84711.000000       0.020023       0.064070       0.182158   \n",
       "75%    139333.000000       1.316218       0.800446       1.028372   \n",
       "max    172792.000000       2.454930      18.902453       9.382558   \n",
       "\n",
       "                  V4             V5             V6             V7  \\\n",
       "count  284315.000000  284315.000000  284315.000000  284315.000000   \n",
       "mean       -0.007860       0.005453       0.002419       0.009637   \n",
       "std         1.399333       1.356952       1.329913       1.178812   \n",
       "min        -5.683171    -113.743307     -26.160506     -31.764946   \n",
       "25%        -0.850077      -0.689398      -0.766847      -0.551442   \n",
       "50%        -0.022405      -0.053457      -0.273123       0.041138   \n",
       "75%         0.737624       0.612181       0.399619       0.571019   \n",
       "max        16.875344      34.801666      73.301626     120.589494   \n",
       "\n",
       "                  V8             V9  ...            V21            V22  \\\n",
       "count  284315.000000  284315.000000  ...  284315.000000  284315.000000   \n",
       "mean       -0.000987       0.004467  ...      -0.001235      -0.000024   \n",
       "std         1.161283       1.089372  ...       0.716743       0.723668   \n",
       "min       -73.216718      -6.290730  ...     -34.830382     -10.933144   \n",
       "25%        -0.208633      -0.640412  ...      -0.228509      -0.542403   \n",
       "50%         0.022041      -0.049964  ...      -0.029821       0.006736   \n",
       "75%         0.326200       0.598230  ...       0.185626       0.528407   \n",
       "max        18.709255      15.594995  ...      22.614889      10.503090   \n",
       "\n",
       "                 V23            V24            V25            V26  \\\n",
       "count  284315.000000  284315.000000  284315.000000  284315.000000   \n",
       "mean        0.000070       0.000182      -0.000072      -0.000089   \n",
       "std         0.621541       0.605776       0.520673       0.482241   \n",
       "min       -44.807735      -2.836627     -10.295397      -2.604551   \n",
       "25%        -0.161702      -0.354425      -0.317145      -0.327074   \n",
       "50%        -0.011147       0.041082       0.016417      -0.052227   \n",
       "75%         0.147522       0.439869       0.350594       0.240671   \n",
       "max        22.528412       4.584549       7.519589       3.517346   \n",
       "\n",
       "                 V27            V28         Amount     Class  \n",
       "count  284315.000000  284315.000000  284315.000000  284315.0  \n",
       "mean       -0.000295      -0.000131      88.291022       0.0  \n",
       "std         0.399847       0.329570     250.105092       0.0  \n",
       "min       -22.565679     -15.430084       0.000000       0.0  \n",
       "25%        -0.070852      -0.052950       5.650000       0.0  \n",
       "50%         0.001230       0.011199      22.000000       0.0  \n",
       "75%         0.090573       0.077962      77.050000       0.0  \n",
       "max        31.612198      33.847808   25691.160000       0.0  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "088e4287-c776-464d-bf4c-ed81c49dc6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>80746.806911</td>\n",
       "      <td>-4.771948</td>\n",
       "      <td>3.623778</td>\n",
       "      <td>-7.033281</td>\n",
       "      <td>4.542029</td>\n",
       "      <td>-3.151225</td>\n",
       "      <td>-1.397737</td>\n",
       "      <td>-5.568731</td>\n",
       "      <td>0.570636</td>\n",
       "      <td>-2.581123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713588</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>-0.040308</td>\n",
       "      <td>-0.105130</td>\n",
       "      <td>0.041449</td>\n",
       "      <td>0.051648</td>\n",
       "      <td>0.170575</td>\n",
       "      <td>0.075667</td>\n",
       "      <td>122.211321</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47835.365138</td>\n",
       "      <td>6.783687</td>\n",
       "      <td>4.291216</td>\n",
       "      <td>7.110937</td>\n",
       "      <td>2.873318</td>\n",
       "      <td>5.372468</td>\n",
       "      <td>1.858124</td>\n",
       "      <td>7.206773</td>\n",
       "      <td>6.797831</td>\n",
       "      <td>2.500896</td>\n",
       "      <td>...</td>\n",
       "      <td>3.869304</td>\n",
       "      <td>1.494602</td>\n",
       "      <td>1.579642</td>\n",
       "      <td>0.515577</td>\n",
       "      <td>0.797205</td>\n",
       "      <td>0.471679</td>\n",
       "      <td>1.376766</td>\n",
       "      <td>0.547291</td>\n",
       "      <td>256.683288</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>406.000000</td>\n",
       "      <td>-30.552380</td>\n",
       "      <td>-8.402154</td>\n",
       "      <td>-31.103685</td>\n",
       "      <td>-1.313275</td>\n",
       "      <td>-22.105532</td>\n",
       "      <td>-6.406267</td>\n",
       "      <td>-43.557242</td>\n",
       "      <td>-41.044261</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.797604</td>\n",
       "      <td>-8.887017</td>\n",
       "      <td>-19.254328</td>\n",
       "      <td>-2.028024</td>\n",
       "      <td>-4.781606</td>\n",
       "      <td>-1.152671</td>\n",
       "      <td>-7.263482</td>\n",
       "      <td>-1.869290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>41241.500000</td>\n",
       "      <td>-6.036063</td>\n",
       "      <td>1.188226</td>\n",
       "      <td>-8.643489</td>\n",
       "      <td>2.373050</td>\n",
       "      <td>-4.792835</td>\n",
       "      <td>-2.501511</td>\n",
       "      <td>-7.965295</td>\n",
       "      <td>-0.195336</td>\n",
       "      <td>-3.872383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041787</td>\n",
       "      <td>-0.533764</td>\n",
       "      <td>-0.342175</td>\n",
       "      <td>-0.436809</td>\n",
       "      <td>-0.314348</td>\n",
       "      <td>-0.259416</td>\n",
       "      <td>-0.020025</td>\n",
       "      <td>-0.108868</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>75568.500000</td>\n",
       "      <td>-2.342497</td>\n",
       "      <td>2.717869</td>\n",
       "      <td>-5.075257</td>\n",
       "      <td>4.177147</td>\n",
       "      <td>-1.522962</td>\n",
       "      <td>-1.424616</td>\n",
       "      <td>-3.034402</td>\n",
       "      <td>0.621508</td>\n",
       "      <td>-2.208768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.592146</td>\n",
       "      <td>0.048434</td>\n",
       "      <td>-0.073135</td>\n",
       "      <td>-0.060795</td>\n",
       "      <td>0.088371</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>0.394926</td>\n",
       "      <td>0.146344</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>128483.000000</td>\n",
       "      <td>-0.419200</td>\n",
       "      <td>4.971257</td>\n",
       "      <td>-2.276185</td>\n",
       "      <td>6.348729</td>\n",
       "      <td>0.214562</td>\n",
       "      <td>-0.413216</td>\n",
       "      <td>-0.945954</td>\n",
       "      <td>1.764879</td>\n",
       "      <td>-0.787850</td>\n",
       "      <td>...</td>\n",
       "      <td>1.244611</td>\n",
       "      <td>0.617474</td>\n",
       "      <td>0.308378</td>\n",
       "      <td>0.285328</td>\n",
       "      <td>0.456515</td>\n",
       "      <td>0.396733</td>\n",
       "      <td>0.826029</td>\n",
       "      <td>0.381152</td>\n",
       "      <td>105.890000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>170348.000000</td>\n",
       "      <td>2.132386</td>\n",
       "      <td>22.057729</td>\n",
       "      <td>2.250210</td>\n",
       "      <td>12.114672</td>\n",
       "      <td>11.095089</td>\n",
       "      <td>6.474115</td>\n",
       "      <td>5.802537</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>3.353525</td>\n",
       "      <td>...</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>8.361985</td>\n",
       "      <td>5.466230</td>\n",
       "      <td>1.091435</td>\n",
       "      <td>2.208209</td>\n",
       "      <td>2.745261</td>\n",
       "      <td>3.052358</td>\n",
       "      <td>1.779364</td>\n",
       "      <td>2125.870000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time          V1          V2          V3          V4  \\\n",
       "count     492.000000  492.000000  492.000000  492.000000  492.000000   \n",
       "mean    80746.806911   -4.771948    3.623778   -7.033281    4.542029   \n",
       "std     47835.365138    6.783687    4.291216    7.110937    2.873318   \n",
       "min       406.000000  -30.552380   -8.402154  -31.103685   -1.313275   \n",
       "25%     41241.500000   -6.036063    1.188226   -8.643489    2.373050   \n",
       "50%     75568.500000   -2.342497    2.717869   -5.075257    4.177147   \n",
       "75%    128483.000000   -0.419200    4.971257   -2.276185    6.348729   \n",
       "max    170348.000000    2.132386   22.057729    2.250210   12.114672   \n",
       "\n",
       "               V5          V6          V7          V8          V9  ...  \\\n",
       "count  492.000000  492.000000  492.000000  492.000000  492.000000  ...   \n",
       "mean    -3.151225   -1.397737   -5.568731    0.570636   -2.581123  ...   \n",
       "std      5.372468    1.858124    7.206773    6.797831    2.500896  ...   \n",
       "min    -22.105532   -6.406267  -43.557242  -41.044261  -13.434066  ...   \n",
       "25%     -4.792835   -2.501511   -7.965295   -0.195336   -3.872383  ...   \n",
       "50%     -1.522962   -1.424616   -3.034402    0.621508   -2.208768  ...   \n",
       "75%      0.214562   -0.413216   -0.945954    1.764879   -0.787850  ...   \n",
       "max     11.095089    6.474115    5.802537   20.007208    3.353525  ...   \n",
       "\n",
       "              V21         V22         V23         V24         V25         V26  \\\n",
       "count  492.000000  492.000000  492.000000  492.000000  492.000000  492.000000   \n",
       "mean     0.713588    0.014049   -0.040308   -0.105130    0.041449    0.051648   \n",
       "std      3.869304    1.494602    1.579642    0.515577    0.797205    0.471679   \n",
       "min    -22.797604   -8.887017  -19.254328   -2.028024   -4.781606   -1.152671   \n",
       "25%      0.041787   -0.533764   -0.342175   -0.436809   -0.314348   -0.259416   \n",
       "50%      0.592146    0.048434   -0.073135   -0.060795    0.088371    0.004321   \n",
       "75%      1.244611    0.617474    0.308378    0.285328    0.456515    0.396733   \n",
       "max     27.202839    8.361985    5.466230    1.091435    2.208209    2.745261   \n",
       "\n",
       "              V27         V28       Amount  Class  \n",
       "count  492.000000  492.000000   492.000000  492.0  \n",
       "mean     0.170575    0.075667   122.211321    1.0  \n",
       "std      1.376766    0.547291   256.683288    0.0  \n",
       "min     -7.263482   -1.869290     0.000000    1.0  \n",
       "25%     -0.020025   -0.108868     1.000000    1.0  \n",
       "50%      0.394926    0.146344     9.250000    1.0  \n",
       "75%      0.826029    0.381152   105.890000    1.0  \n",
       "max      3.052358    1.779364  2125.870000    1.0  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "446320a1-19cd-456c-8191-95db7d1cddd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94838.202258</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>-0.006271</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>-0.007860</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>0.009637</td>\n",
       "      <td>-0.000987</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000644</td>\n",
       "      <td>-0.001235</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.000295</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>88.291022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80746.806911</td>\n",
       "      <td>-4.771948</td>\n",
       "      <td>3.623778</td>\n",
       "      <td>-7.033281</td>\n",
       "      <td>4.542029</td>\n",
       "      <td>-3.151225</td>\n",
       "      <td>-1.397737</td>\n",
       "      <td>-5.568731</td>\n",
       "      <td>0.570636</td>\n",
       "      <td>-2.581123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372319</td>\n",
       "      <td>0.713588</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>-0.040308</td>\n",
       "      <td>-0.105130</td>\n",
       "      <td>0.041449</td>\n",
       "      <td>0.051648</td>\n",
       "      <td>0.170575</td>\n",
       "      <td>0.075667</td>\n",
       "      <td>122.211321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Time        V1        V2        V3        V4        V5  \\\n",
       "Class                                                                   \n",
       "0      94838.202258  0.008258 -0.006271  0.012171 -0.007860  0.005453   \n",
       "1      80746.806911 -4.771948  3.623778 -7.033281  4.542029 -3.151225   \n",
       "\n",
       "             V6        V7        V8        V9  ...       V20       V21  \\\n",
       "Class                                          ...                       \n",
       "0      0.002419  0.009637 -0.000987  0.004467  ... -0.000644 -0.001235   \n",
       "1     -1.397737 -5.568731  0.570636 -2.581123  ...  0.372319  0.713588   \n",
       "\n",
       "            V22       V23       V24       V25       V26       V27       V28  \\\n",
       "Class                                                                         \n",
       "0     -0.000024  0.000070  0.000182 -0.000072 -0.000089 -0.000295 -0.000131   \n",
       "1      0.014049 -0.040308 -0.105130  0.041449  0.051648  0.170575  0.075667   \n",
       "\n",
       "           Amount  \n",
       "Class              \n",
       "0       88.291022  \n",
       "1      122.211321  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Class').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f8d10-04fc-4c3a-8c17-380d4037002a",
   "metadata": {},
   "source": [
    "As data is highly unbalanced we need to balance it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db59071b-1c9d-4bea-8f5c-efd168712115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import  KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import  MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235c8ff-51e4-4cb3-bd35-29d06e45609f",
   "metadata": {},
   "source": [
    "Without Feature Selection and without data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "479a4c20-23a2-433c-ab70-d42f18ed3015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train :  (213605, 30)\n",
      "X Test  :  (71202, 30)\n",
      "Y Train :  (213605,)\n",
      "Y Test  :  (71202,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=\"Class\")           \n",
    "y = df[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"X Train : \", X_train.shape)\n",
    "print(\"X Test  : \", X_test.shape)\n",
    "print(\"Y Train : \", y_train.shape)\n",
    "print(\"Y Test  : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d44cd774-a77c-4aaa-8096-689b68042770",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "DTR = DecisionTreeClassifier()\n",
    "RFR = RandomForestClassifier()\n",
    "KNR = KNeighborsClassifier()\n",
    "MLP = MLPClassifier()\n",
    "SVC=SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d0205ed-87b5-45b9-8a8a-b3712f0429ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\ML\\credit_fraud_detection\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       0.61      0.59      0.60       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.81      0.80      0.80     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n",
      "DecisionTreeClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       0.71      0.74      0.72       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.85      0.87      0.86     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n",
      "RandomForestClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       0.92      0.80      0.85       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.96      0.90      0.93     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n",
      "KNeighborsClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       1.00      0.06      0.12       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       1.00      0.53      0.56     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n",
      "MLPClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       0.60      0.03      0.05       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.80      0.51      0.53     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n",
      "SVC() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       0.00      0.00      0.00       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.50      0.50      0.50     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\ML\\credit_fraud_detection\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\projects\\ML\\credit_fraud_detection\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\projects\\ML\\credit_fraud_detection\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "LR.fit(X_train,y_train)\n",
    "ypred = LR.predict(X_test)\n",
    "print(LR,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "DTR.fit(X_train,y_train)\n",
    "ypred = DTR.predict(X_test)\n",
    "print(DTR,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "RFR.fit(X_train,y_train)\n",
    "ypred = RFR.predict(X_test)\n",
    "print(RFR,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "KNR.fit(X_train,y_train)\n",
    "ypred = KNR.predict(X_test)\n",
    "print(KNR,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "MLP.fit(X_train,y_train)\n",
    "ypred = MLP.predict(X_test)\n",
    "print(MLP,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "SVC.fit(X_train,y_train)\n",
    "ypred = SVC.predict(X_test)\n",
    "print(SVC,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1216bd7d-8799-4621-aacb-52396970f974",
   "metadata": {},
   "source": [
    "With Feature Selection and without data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d539e1b-6d51-4080-85f9-85f09c4a42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "297af530-76cd-421a-9152-f99b61d2e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_50 = SelectPercentile( percentile= 50)\n",
    "set_X = F_50.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "111f12c1-cc1f-4ae9-a229-acc5888b22d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>-1.593105</td>\n",
       "      <td>2.711941</td>\n",
       "      <td>4.626942</td>\n",
       "      <td>1.107641</td>\n",
       "      <td>1.991691</td>\n",
       "      <td>0.510632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>-0.150189</td>\n",
       "      <td>0.915802</td>\n",
       "      <td>-0.675143</td>\n",
       "      <td>-0.711757</td>\n",
       "      <td>-0.025693</td>\n",
       "      <td>-1.221179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>0.411614</td>\n",
       "      <td>0.063119</td>\n",
       "      <td>-0.510602</td>\n",
       "      <td>0.140716</td>\n",
       "      <td>0.313502</td>\n",
       "      <td>0.395652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>-1.933849</td>\n",
       "      <td>-0.962886</td>\n",
       "      <td>0.449624</td>\n",
       "      <td>-0.608577</td>\n",
       "      <td>0.509928</td>\n",
       "      <td>1.113981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>-1.040458</td>\n",
       "      <td>-0.031513</td>\n",
       "      <td>-0.084316</td>\n",
       "      <td>-0.302620</td>\n",
       "      <td>-0.660377</td>\n",
       "      <td>0.167430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1         V2        V3        V4        V5        V6  \\\n",
       "0       -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1        1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "284802 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "284803  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "284804   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "284805  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "284806  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V9       V10       V11       V12       V14       V16  \\\n",
       "0       0.239599  0.363787  0.090794 -0.551600 -0.617801 -0.311169 -0.470401   \n",
       "1      -0.078803 -0.255425 -0.166974  1.612727  1.065235 -0.143772  0.463917   \n",
       "2       0.791461 -1.514654  0.207643  0.624501  0.066084 -0.165946 -2.890083   \n",
       "3       0.237609 -1.387024 -0.054952 -0.226487  0.178228 -0.287924 -1.059647   \n",
       "4       0.592941  0.817739  0.753074 -0.822843  0.538196 -1.119670 -0.451449   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802 -4.918215  1.914428  4.356170 -1.593105  2.711941  4.626942  1.107641   \n",
       "284803  0.024330  0.584800 -0.975926 -0.150189  0.915802 -0.675143 -0.711757   \n",
       "284804 -0.296827  0.432454 -0.484782  0.411614  0.063119 -0.510602  0.140716   \n",
       "284805 -0.686180  0.392087 -0.399126 -1.933849 -0.962886  0.449624 -0.608577   \n",
       "284806  1.577006  0.486180 -0.915427 -1.040458 -0.031513 -0.084316 -0.302620   \n",
       "\n",
       "             V17       V18  \n",
       "0       0.207971  0.025791  \n",
       "1      -0.114805 -0.183361  \n",
       "2       1.109969 -0.121359  \n",
       "3      -0.684093  1.965775  \n",
       "4      -0.237033 -0.038195  \n",
       "...          ...       ...  \n",
       "284802  1.991691  0.510632  \n",
       "284803 -0.025693 -1.221179  \n",
       "284804  0.313502  0.395652  \n",
       "284805  0.509928  1.113981  \n",
       "284806 -0.660377  0.167430  \n",
       "\n",
       "[284807 rows x 15 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FS_x =pd.DataFrame(set_X,columns= [ i for i,j in zip (X.columns, F_50.get_support()) if j == True ])\n",
    "FS_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7105096-589c-461a-a019-de5adf834fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FS_x      \n",
    "y = df[\"Class\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a969ccd3-83d3-4deb-8f67-f215ab39218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       0.83      0.59      0.69       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.91      0.80      0.85     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n",
      "DecisionTreeClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       0.71      0.78      0.74       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.85      0.89      0.87     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n",
      "RandomForestClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       0.93      0.77      0.84       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.96      0.88      0.92     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n",
      "KNeighborsClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       0.87      0.80      0.83       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.94      0.90      0.92     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n",
      "MLPClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       0.86      0.73      0.79       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.93      0.87      0.90     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n",
      "SVC() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71089\n",
      "           1       0.92      0.68      0.78       113\n",
      "\n",
      "    accuracy                           1.00     71202\n",
      "   macro avg       0.96      0.84      0.89     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      "\n",
      "********************************************\n"
     ]
    }
   ],
   "source": [
    "LR.fit(X_train,y_train)\n",
    "ypred = LR.predict(X_test)\n",
    "print(LR,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "DTR.fit(X_train,y_train)\n",
    "ypred = DTR.predict(X_test)\n",
    "print(DTR,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "RFR.fit(X_train,y_train)\n",
    "ypred = RFR.predict(X_test)\n",
    "print(RFR,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "KNR.fit(X_train,y_train)\n",
    "ypred = KNR.predict(X_test)\n",
    "print(KNR,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "MLP.fit(X_train,y_train)\n",
    "ypred = MLP.predict(X_test)\n",
    "print(MLP,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "SVC.fit(X_train,y_train)\n",
    "ypred = SVC.predict(X_test)\n",
    "print(SVC,\":\\n\",classification_report(y_test,ypred))\n",
    "print(\"********************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f497f-2575-425a-ac38-cec894c44115",
   "metadata": {},
   "source": [
    "This data is highly unbalanced so the best metric to calculate efficiency is f1-score for 1.\n",
    "You can see the f1-score for 1 increased after using feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c37438-73cd-4ca2-a515-80e785c6dca6",
   "metadata": {},
   "source": [
    "Random Forest Classifier is giving the best f1-score for 1 of 0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799bb0b-18b4-4482-98d1-5034a812b0ea",
   "metadata": {},
   "source": [
    "With Feature Selection and with data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ae0ada2-0b47-4a2d-bf44-5214a1301676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259610</th>\n",
       "      <td>159196.0</td>\n",
       "      <td>-1.905451</td>\n",
       "      <td>2.272360</td>\n",
       "      <td>-1.382722</td>\n",
       "      <td>0.869588</td>\n",
       "      <td>-0.370347</td>\n",
       "      <td>-0.856581</td>\n",
       "      <td>0.020436</td>\n",
       "      <td>1.313534</td>\n",
       "      <td>-0.450615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172948</td>\n",
       "      <td>0.468436</td>\n",
       "      <td>-0.114319</td>\n",
       "      <td>-0.047389</td>\n",
       "      <td>0.110792</td>\n",
       "      <td>-0.369387</td>\n",
       "      <td>0.367603</td>\n",
       "      <td>0.209285</td>\n",
       "      <td>5.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244584</th>\n",
       "      <td>152410.0</td>\n",
       "      <td>2.052646</td>\n",
       "      <td>-0.102170</td>\n",
       "      <td>-1.078682</td>\n",
       "      <td>0.414843</td>\n",
       "      <td>-0.164990</td>\n",
       "      <td>-1.142989</td>\n",
       "      <td>0.130530</td>\n",
       "      <td>-0.275853</td>\n",
       "      <td>0.628869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.290023</td>\n",
       "      <td>-0.723641</td>\n",
       "      <td>0.353823</td>\n",
       "      <td>-0.029285</td>\n",
       "      <td>-0.333379</td>\n",
       "      <td>0.203009</td>\n",
       "      <td>-0.071432</td>\n",
       "      <td>-0.061725</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8223</th>\n",
       "      <td>11055.0</td>\n",
       "      <td>1.189239</td>\n",
       "      <td>-0.025485</td>\n",
       "      <td>0.985957</td>\n",
       "      <td>0.256872</td>\n",
       "      <td>-0.854403</td>\n",
       "      <td>-0.627561</td>\n",
       "      <td>-0.517071</td>\n",
       "      <td>-0.084322</td>\n",
       "      <td>1.424481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.240627</td>\n",
       "      <td>-0.005876</td>\n",
       "      <td>0.559473</td>\n",
       "      <td>0.157949</td>\n",
       "      <td>1.000326</td>\n",
       "      <td>-0.091274</td>\n",
       "      <td>-0.001760</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143429</th>\n",
       "      <td>85347.0</td>\n",
       "      <td>-0.806924</td>\n",
       "      <td>0.985300</td>\n",
       "      <td>1.083525</td>\n",
       "      <td>-1.176312</td>\n",
       "      <td>-0.678492</td>\n",
       "      <td>-1.167162</td>\n",
       "      <td>0.097048</td>\n",
       "      <td>0.399496</td>\n",
       "      <td>0.139810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057487</td>\n",
       "      <td>-0.344062</td>\n",
       "      <td>0.038460</td>\n",
       "      <td>0.400127</td>\n",
       "      <td>-0.274599</td>\n",
       "      <td>0.715132</td>\n",
       "      <td>-0.092635</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85975</th>\n",
       "      <td>61022.0</td>\n",
       "      <td>-0.371806</td>\n",
       "      <td>0.819405</td>\n",
       "      <td>2.134793</td>\n",
       "      <td>0.563418</td>\n",
       "      <td>-0.203519</td>\n",
       "      <td>-0.691185</td>\n",
       "      <td>1.033441</td>\n",
       "      <td>-0.330350</td>\n",
       "      <td>-0.423058</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100286</td>\n",
       "      <td>-0.220377</td>\n",
       "      <td>0.073508</td>\n",
       "      <td>0.671890</td>\n",
       "      <td>-0.187955</td>\n",
       "      <td>-0.717490</td>\n",
       "      <td>-0.087783</td>\n",
       "      <td>-0.138918</td>\n",
       "      <td>59.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "259610  159196.0 -1.905451  2.272360 -1.382722  0.869588 -0.370347 -0.856581   \n",
       "244584  152410.0  2.052646 -0.102170 -1.078682  0.414843 -0.164990 -1.142989   \n",
       "8223     11055.0  1.189239 -0.025485  0.985957  0.256872 -0.854403 -0.627561   \n",
       "143429   85347.0 -0.806924  0.985300  1.083525 -1.176312 -0.678492 -1.167162   \n",
       "85975    61022.0 -0.371806  0.819405  2.134793  0.563418 -0.203519 -0.691185   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "259610  0.020436  1.313534 -0.450615  ...  0.172948  0.468436 -0.114319   \n",
       "244584  0.130530 -0.275853  0.628869  ... -0.290023 -0.723641  0.353823   \n",
       "8223   -0.517071 -0.084322  1.424481  ...  0.001630  0.240627 -0.005876   \n",
       "143429  0.097048  0.399496  0.139810  ... -0.057487 -0.344062  0.038460   \n",
       "85975   1.033441 -0.330350 -0.423058  ... -0.100286 -0.220377  0.073508   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "259610 -0.047389  0.110792 -0.369387  0.367603  0.209285    5.99      0  \n",
       "244584 -0.029285 -0.333379  0.203009 -0.071432 -0.061725    1.98      0  \n",
       "8223    0.559473  0.157949  1.000326 -0.091274 -0.001760   15.95      0  \n",
       "143429  0.400127 -0.274599  0.715132 -0.092635  0.001381    0.23      0  \n",
       "85975   0.671890 -0.187955 -0.717490 -0.087783 -0.138918   59.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legit_sample = legit.sample(n=492)\n",
    "balanced_df = pd.concat([legit_sample, fraud], axis=0)\n",
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4ed159e-8378-4898-bc55-b72b6fe6880b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    492\n",
       "1    492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4048d033-9949-4c47-b587-b32adb578235",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_X = balanced_df.drop(columns=\"Class\")           \n",
    "b_y = balanced_df[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ead813c-21a4-4c75-94b3-f6e9ec84f418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V14</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.905451</td>\n",
       "      <td>2.272360</td>\n",
       "      <td>-1.382722</td>\n",
       "      <td>0.869588</td>\n",
       "      <td>-0.370347</td>\n",
       "      <td>-0.856581</td>\n",
       "      <td>0.020436</td>\n",
       "      <td>-0.450615</td>\n",
       "      <td>0.504955</td>\n",
       "      <td>-0.137545</td>\n",
       "      <td>-0.281883</td>\n",
       "      <td>1.960600</td>\n",
       "      <td>-0.611241</td>\n",
       "      <td>0.606161</td>\n",
       "      <td>0.489447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.052646</td>\n",
       "      <td>-0.102170</td>\n",
       "      <td>-1.078682</td>\n",
       "      <td>0.414843</td>\n",
       "      <td>-0.164990</td>\n",
       "      <td>-1.142989</td>\n",
       "      <td>0.130530</td>\n",
       "      <td>0.628869</td>\n",
       "      <td>0.081963</td>\n",
       "      <td>-0.790759</td>\n",
       "      <td>0.146365</td>\n",
       "      <td>0.397963</td>\n",
       "      <td>-0.136222</td>\n",
       "      <td>-0.198135</td>\n",
       "      <td>-0.886500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.189239</td>\n",
       "      <td>-0.025485</td>\n",
       "      <td>0.985957</td>\n",
       "      <td>0.256872</td>\n",
       "      <td>-0.854403</td>\n",
       "      <td>-0.627561</td>\n",
       "      <td>-0.517071</td>\n",
       "      <td>1.424481</td>\n",
       "      <td>-0.358949</td>\n",
       "      <td>2.566880</td>\n",
       "      <td>-1.475165</td>\n",
       "      <td>1.633470</td>\n",
       "      <td>0.611693</td>\n",
       "      <td>0.103721</td>\n",
       "      <td>0.552376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.806924</td>\n",
       "      <td>0.985300</td>\n",
       "      <td>1.083525</td>\n",
       "      <td>-1.176312</td>\n",
       "      <td>-0.678492</td>\n",
       "      <td>-1.167162</td>\n",
       "      <td>0.097048</td>\n",
       "      <td>0.139810</td>\n",
       "      <td>-0.744806</td>\n",
       "      <td>-0.751822</td>\n",
       "      <td>-0.325939</td>\n",
       "      <td>0.377321</td>\n",
       "      <td>0.799733</td>\n",
       "      <td>-0.479000</td>\n",
       "      <td>-0.233291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.371806</td>\n",
       "      <td>0.819405</td>\n",
       "      <td>2.134793</td>\n",
       "      <td>0.563418</td>\n",
       "      <td>-0.203519</td>\n",
       "      <td>-0.691185</td>\n",
       "      <td>1.033441</td>\n",
       "      <td>-0.423058</td>\n",
       "      <td>-0.348113</td>\n",
       "      <td>0.021603</td>\n",
       "      <td>0.385312</td>\n",
       "      <td>-0.218904</td>\n",
       "      <td>0.062862</td>\n",
       "      <td>-0.563684</td>\n",
       "      <td>-0.231126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>-2.064945</td>\n",
       "      <td>-5.587794</td>\n",
       "      <td>2.115795</td>\n",
       "      <td>-5.417424</td>\n",
       "      <td>-6.665177</td>\n",
       "      <td>-2.897825</td>\n",
       "      <td>-4.570529</td>\n",
       "      <td>-1.315147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>1.378559</td>\n",
       "      <td>1.289381</td>\n",
       "      <td>-5.004247</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>-1.326536</td>\n",
       "      <td>-1.413170</td>\n",
       "      <td>-1.127396</td>\n",
       "      <td>-3.232153</td>\n",
       "      <td>2.858466</td>\n",
       "      <td>-3.096915</td>\n",
       "      <td>-5.210141</td>\n",
       "      <td>-2.155297</td>\n",
       "      <td>-3.267116</td>\n",
       "      <td>-0.688505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>-0.676143</td>\n",
       "      <td>1.126366</td>\n",
       "      <td>-2.213700</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>-1.120541</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-2.234739</td>\n",
       "      <td>-0.652250</td>\n",
       "      <td>-3.463891</td>\n",
       "      <td>1.794969</td>\n",
       "      <td>-2.775022</td>\n",
       "      <td>-4.057162</td>\n",
       "      <td>-1.603015</td>\n",
       "      <td>-5.035326</td>\n",
       "      <td>-0.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>-3.113832</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>-5.399730</td>\n",
       "      <td>1.817092</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>-2.943548</td>\n",
       "      <td>-2.208002</td>\n",
       "      <td>-1.632333</td>\n",
       "      <td>-5.245984</td>\n",
       "      <td>1.933520</td>\n",
       "      <td>-5.030465</td>\n",
       "      <td>-6.416628</td>\n",
       "      <td>-2.549498</td>\n",
       "      <td>-4.614717</td>\n",
       "      <td>-1.478138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>1.991976</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>-2.583441</td>\n",
       "      <td>0.408670</td>\n",
       "      <td>1.151147</td>\n",
       "      <td>-0.096695</td>\n",
       "      <td>0.223050</td>\n",
       "      <td>0.577829</td>\n",
       "      <td>-0.888722</td>\n",
       "      <td>0.491140</td>\n",
       "      <td>0.728903</td>\n",
       "      <td>-1.948883</td>\n",
       "      <td>0.519436</td>\n",
       "      <td>0.903562</td>\n",
       "      <td>1.197315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   -1.905451  2.272360 -1.382722  0.869588 -0.370347 -0.856581  0.020436   \n",
       "1    2.052646 -0.102170 -1.078682  0.414843 -0.164990 -1.142989  0.130530   \n",
       "2    1.189239 -0.025485  0.985957  0.256872 -0.854403 -0.627561 -0.517071   \n",
       "3   -0.806924  0.985300  1.083525 -1.176312 -0.678492 -1.167162  0.097048   \n",
       "4   -0.371806  0.819405  2.134793  0.563418 -0.203519 -0.691185  1.033441   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "979 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494 -0.882850   \n",
       "980  1.378559  1.289381 -5.004247  1.411850  0.442581 -1.326536 -1.413170   \n",
       "981 -0.676143  1.126366 -2.213700  0.468308 -1.120541 -0.003346 -2.234739   \n",
       "982 -3.113832  0.585864 -5.399730  1.817092 -0.840618 -2.943548 -2.208002   \n",
       "983  1.991976  0.158476 -2.583441  0.408670  1.151147 -0.096695  0.223050   \n",
       "\n",
       "           V9       V10       V11       V12       V14       V16       V17  \\\n",
       "0   -0.450615  0.504955 -0.137545 -0.281883  1.960600 -0.611241  0.606161   \n",
       "1    0.628869  0.081963 -0.790759  0.146365  0.397963 -0.136222 -0.198135   \n",
       "2    1.424481 -0.358949  2.566880 -1.475165  1.633470  0.611693  0.103721   \n",
       "3    0.139810 -0.744806 -0.751822 -0.325939  0.377321  0.799733 -0.479000   \n",
       "4   -0.423058 -0.348113  0.021603  0.385312 -0.218904  0.062862 -0.563684   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "979 -2.064945 -5.587794  2.115795 -5.417424 -6.665177 -2.897825 -4.570529   \n",
       "980 -1.127396 -3.232153  2.858466 -3.096915 -5.210141 -2.155297 -3.267116   \n",
       "981 -0.652250 -3.463891  1.794969 -2.775022 -4.057162 -1.603015 -5.035326   \n",
       "982 -1.632333 -5.245984  1.933520 -5.030465 -6.416628 -2.549498 -4.614717   \n",
       "983  0.577829 -0.888722  0.491140  0.728903 -1.948883  0.519436  0.903562   \n",
       "\n",
       "          V18  \n",
       "0    0.489447  \n",
       "1   -0.886500  \n",
       "2    0.552376  \n",
       "3   -0.233291  \n",
       "4   -0.231126  \n",
       "..        ...  \n",
       "979 -1.315147  \n",
       "980 -0.688505  \n",
       "981 -0.507000  \n",
       "982 -1.478138  \n",
       "983  1.197315  \n",
       "\n",
       "[984 rows x 15 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_balanced_df = F_50.fit_transform(b_X,b_y)\n",
    "FS_balanced_df = pd.DataFrame(set_balanced_df ,columns= [ i for i,j in zip (b_X.columns, F_50.get_support()) if j == True ])\n",
    "FS_balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47a35618-fb70-4673-ab9a-b0850b7f2cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train :  (738, 15)\n",
      "X Test  :  (246, 15)\n",
      "Y Train :  (738,)\n",
      "Y Test  :  (246,)\n"
     ]
    }
   ],
   "source": [
    "b_X = FS_balanced_df\n",
    "\n",
    "b_X_train, b_X_test, b_y_train, b_y_test = train_test_split(b_X, b_y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"X Train : \", b_X_train.shape)\n",
    "print(\"X Test  : \", b_X_test.shape)\n",
    "print(\"Y Train : \", b_y_train.shape)\n",
    "print(\"Y Test  : \", b_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5fe35a9a-c154-48d1-8ddf-b1969c6e5471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       126\n",
      "           1       0.96      0.88      0.92       120\n",
      "\n",
      "    accuracy                           0.92       246\n",
      "   macro avg       0.93      0.92      0.92       246\n",
      "weighted avg       0.93      0.92      0.92       246\n",
      "\n",
      "********************************************\n",
      "DecisionTreeClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       126\n",
      "           1       0.91      0.89      0.90       120\n",
      "\n",
      "    accuracy                           0.91       246\n",
      "   macro avg       0.91      0.91      0.91       246\n",
      "weighted avg       0.91      0.91      0.91       246\n",
      "\n",
      "********************************************\n",
      "RandomForestClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92       126\n",
      "           1       0.95      0.88      0.91       120\n",
      "\n",
      "    accuracy                           0.92       246\n",
      "   macro avg       0.92      0.92      0.92       246\n",
      "weighted avg       0.92      0.92      0.92       246\n",
      "\n",
      "********************************************\n",
      "KNeighborsClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93       126\n",
      "           1       0.97      0.87      0.92       120\n",
      "\n",
      "    accuracy                           0.92       246\n",
      "   macro avg       0.93      0.92      0.92       246\n",
      "weighted avg       0.93      0.92      0.92       246\n",
      "\n",
      "********************************************\n",
      "MLPClassifier() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.94       126\n",
      "           1       0.98      0.89      0.93       120\n",
      "\n",
      "    accuracy                           0.94       246\n",
      "   macro avg       0.94      0.94      0.94       246\n",
      "weighted avg       0.94      0.94      0.94       246\n",
      "\n",
      "********************************************\n",
      "SVC() :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92       126\n",
      "           1       0.95      0.87      0.91       120\n",
      "\n",
      "    accuracy                           0.91       246\n",
      "   macro avg       0.92      0.91      0.91       246\n",
      "weighted avg       0.92      0.91      0.91       246\n",
      "\n",
      "********************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\ML\\credit_fraud_detection\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "LR.fit(b_X_train,b_y_train)\n",
    "ypred = LR.predict(b_X_test)\n",
    "print(LR,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "DTR.fit(b_X_train,b_y_train)\n",
    "ypred = DTR.predict(b_X_test)\n",
    "print(DTR,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "RFR.fit(b_X_train,b_y_train)\n",
    "ypred = RFR.predict(b_X_test)\n",
    "print(RFR,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "KNR.fit(b_X_train,b_y_train)\n",
    "ypred = KNR.predict(b_X_test)\n",
    "print(KNR,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "MLP.fit(b_X_train,b_y_train)\n",
    "ypred = MLP.predict(b_X_test)\n",
    "print(MLP,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "SVC.fit(b_X_train,b_y_train)\n",
    "ypred = SVC.predict(b_X_test)\n",
    "print(SVC,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b015ff81-dea5-4ced-935b-143a1feedb50",
   "metadata": {},
   "source": [
    "As we can see that f1-score for class 1 has increased after balancing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5531d90-9dd6-4da9-be9e-c1ac58833750",
   "metadata": {},
   "source": [
    "MLP Classifier is giving the best f1-score for 1 of 0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b4ca6-d2f2-4fcd-bb8a-c5940acb90ad",
   "metadata": {},
   "source": [
    "Without Feature Selection and with data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9e4e5227-6236-4688-94f2-463558fc424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_X = balanced_df.drop(columns=\"Class\")           \n",
    "b_y = balanced_df[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "655d7b37-47a3-4916-9997-f8a3e966b1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train :  (738, 30)\n",
      "X Test  :  (246, 30)\n",
      "Y Train :  (738,)\n",
      "Y Test  :  (246,)\n"
     ]
    }
   ],
   "source": [
    "b_X_train, b_X_test, b_y_train, b_y_test = train_test_split(b_X, b_y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"X Train : \", b_X_train.shape)\n",
    "print(\"X Test  : \", b_X_test.shape)\n",
    "print(\"Y Train : \", b_y_train.shape)\n",
    "print(\"Y Test  : \", b_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b642c-65b0-470a-8fc5-94b08c9fed67",
   "metadata": {},
   "source": [
    "LR.fit(b_X_train,b_y_train)\n",
    "ypred = LR.predict(b_X_test)\n",
    "print(LR,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "DTR.fit(b_X_train,b_y_train)\n",
    "ypred = DTR.predict(b_X_test)\n",
    "print(DTR,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "RFR.fit(b_X_train,b_y_train)\n",
    "ypred = RFR.predict(b_X_test)\n",
    "print(RFR,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "KNR.fit(b_X_train,b_y_train)\n",
    "ypred = KNR.predict(b_X_test)\n",
    "print(KNR,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "MLP.fit(b_X_train,b_y_train)\n",
    "ypred = MLP.predict(b_X_test)\n",
    "print(MLP,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")\n",
    "\n",
    "SVC.fit(b_X_train,b_y_train)\n",
    "ypred = SVC.predict(b_X_test)\n",
    "print(SVC,\":\\n\",classification_report(b_y_test,ypred))\n",
    "print(\"********************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8647931-f548-4a87-9586-1cc0bb4f0039",
   "metadata": {},
   "source": [
    "In conclusion, feature extraction helped in KNeighbors, MLP and SVC but did not make much difference in Logistic Regression, Decision Tree and Random Forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d5ccb3-eb24-4be9-9b38-cc409987939f",
   "metadata": {},
   "source": [
    "Finally with feature selection and data balancing MLP classifier gave us the best results "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
